from typing import List, Tuple
import os
import glob
import re
import pickle
import faiss
from langchain.docstore.document import Document
from langchain_experimental.text_splitter import SemanticChunker
from sentence_transformers import SentenceTransformer


def load_markdown_files(folder_path: str) -> List[Tuple[str, str]]:
    """
    ë§ˆí¬ ë‹¤ìš´ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬(íŒŒì¼ëª…, í…ìŠ¤íŠ¸) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """

    file_paths = glob.glob(os.path.join(folder_path, "*.md"))

    return [(path, open(path, "r", encoding="utf-8").read()) for path in file_paths]


def custom_md_splitter(md_text: str) -> List[Document]:
    """
    ë§ˆí¬ë‹¤ìš´ í—¤ë”ê°€ í˜„ì¬ md íŒŒì¼ì— ì—†ê¸° ë•Œë¬¸ì— ì±•í„° êµ¬ë¶„ ë³„ë¡œ (ì˜ˆ : 1. ëª©ì  2. ë°°ê²½)
    ë‚˜ëˆ„ê¸° ìœ„í•œ í•¨ìˆ˜.
    """
    pattern = re.compile(r"(?P<header>^\d+(\.\d+)*\s+.+)", re.MULTILINE)
    matches = list(pattern.finditer(md_text))

    documents = []
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(md_text)

        header = match.group("header").strip()
        content = md_text[start:end].strip()

        # ì»¨í…ì¸ ì—ëŠ” í—¤ë”ë¡œ ë¶„ë¦¬í•œ ê²ƒë„ ë‚´ìš©ê³¼ í•©ì¹˜ê³ , ë©”íƒ€ë°ì´í„°ì—ì„œ "section" ìœ¼ë¡œ êµ¬ë¶„
        if content:
            doc = Document(
                page_content=f"{header}\n{content}", metadata={"section": header}
            )
            documents.append(doc)

    return documents


def semantic_chunk_documents(
    documents: List[Document], model: SentenceTransformer
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹í•˜ëŠ” í•¨ìˆ˜.
    """
    chunker = SemanticChunker(embeddings=model)
    final_chunks = []
    for doc in documents:
        chunks = chunker.split_text(doc.page_content)
        final_chunks.extend(chunks)
    return final_chunks


def load_existing_faiss(output_dir: str):
    """
    ê¸°ì¡´ ì €ì¥ëœ ë²¡í„°DB ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    faiss_path = os.path.join(output_dir, "faiss.index")
    meta_path = os.path.join(output_dir, "metadata.pkl")

    if os.path.exists(faiss_path) and os.path.exists(meta_path):
        index = faiss.read_index(faiss_path)
        with open(meta_path, "rb") as f:
            metadata = pickle.load(f)
        # ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
        existing_paths = set([line.split(" | ")[0] for line in metadata])
        return index, metadata, existing_paths
    else:
        return None, [], set()


def save_faiss(index, metadata: List[str], output_dir: str):
    """
    ë²¡í„°DB ì €ì¥ í•¨ìˆ˜
    """
    os.makedirs(output_dir, exist_ok=True)
    faiss.write_index(index, os.path.join(output_dir, "faiss.index"))
    with open(os.path.join(output_dir, "metadata.pkl"), "wb") as f:
        pickle.dump(metadata, f)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")


def process_md_folder(
    md_folder_path: str,
    output_dir: str,
    model_name="intfloat/multilingual-e5-large",
):
    """
    ìµœì¢…ì ìœ¼ë¡œ ë²¡í„°DBë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜. ì…ë ¥í•œ ê²½ë¡œì— ê¸°ì¡´ ë²¡í„°DBê°€ ì—†ë‹¤ë©´
    ìƒˆë¡œ ìƒì„±í•œë‹¤.

    # íŒŒë¼ë©”í„°
    - md_folder_path : md íŒŒì¼ì´ ëª¨ì—¬ìˆëŠ” í´ë” ì´ë¦„ ì…ë ¥
    - output_dir : ê¸°ì¡´ ì €ì¥ë˜ì–´ ì‡ëŠ” ë²¡í„° (md : vectordb/faiss_md_index)
    """
    model = SentenceTransformer(model_name)
    md_files = load_markdown_files(md_folder_path)

    # ê¸°ì¡´ ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„°, íŒŒì¼ê²½ë¡œ ëª©ë¡ ë¡œë“œ
    index, metadata, existing_paths = load_existing_faiss(output_dir)
    if index is None:
        print("ğŸ”„ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        index = faiss.IndexFlatL2(model.get_sentence_embedding_dimension())
        metadata = []
        existing_paths = set()

    new_chunks = []
    new_metadata = []

    for path, text in md_files:
        if path in existing_paths:
            print(f"â­ ì´ë¯¸ ì²˜ë¦¬ë¨: {path}")
            continue

        docs = custom_md_splitter(text)
        chunks = semantic_chunk_documents(docs, model)
        print(f"ğŸ“„ {os.path.basename(path)} â†’ {len(chunks)} chunks")

        new_chunks.extend(chunks)
        new_metadata.extend([f"{path} | {chunk[:50]}" for chunk in chunks])

    if new_chunks:
        embeddings = model.encode(new_chunks, show_progress_bar=True)
        index.add(embeddings)
        metadata.extend(new_metadata)
        save_faiss(index, metadata, output_dir)
    else:
        print("âœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


def search_md_faiss(
    query: str,
    index_dir: str,
    top_k: int = 5,
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> List[Tuple[str, float]]:
    """
    ë§ˆí¬ë‹¤ìš´ ë²¡í„°DBì— ì§ˆì˜(query)ë¥¼ ê²€ìƒ‰í•˜ê³  ìœ ì‚¬í•œ ì²­í¬ ë°˜í™˜

    Returns: List of (chunk text, score)
    """
    index_path = os.path.join(index_dir, "faiss.index")
    meta_path = os.path.join(index_dir, "metadata.pkl")

    # íŒŒì¼ ì²´í¬
    if not os.path.exists(index_path) or not os.path.exists(meta_path):
        raise FileNotFoundError(
            "FAISS index or metadata.pkl not found in the directory."
        )

    # ë¡œë”©
    index = faiss.read_index(index_path)
    with open(meta_path, "rb") as f:
        metadata = pickle.load(f)

    # ì§ˆì˜ ì„ë² ë”©
    model = SentenceTransformer(model_name)
    query_vector = model.encode([query])

    # ê²€ìƒ‰
    distances, indices = index.search(query_vector, top_k)
    results = []
    for i, score in zip(indices[0], distances[0]):
        if i < len(metadata):
            results.append((metadata[i], score))

    return results
